{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMOsAWv888pLp5DOZjFwEDC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/park-jiha/deep-learning/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25DgbVxXpIY9"
      },
      "source": [
        "# 원-핫 인코딩\r\n",
        "**토큰화 & 인덱스값 출력**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_udw5FkoWHy",
        "outputId": "34c3235a-c14a-42b7-e99b-359b1b86cad1"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "\r\n",
        "text = \"오랫동안 꿈꾸는 이는 그 꿈을 닮아간다\"\r\n",
        "\r\n",
        "token = Tokenizer()\r\n",
        "token.fit_on_texts([text])\r\n",
        "print(token.word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'오랫동안': 1, '꿈꾸는': 2, '이는': 3, '그': 4, '꿈을': 5, '닮아간다': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDgNPEtcpQ5o"
      },
      "source": [
        "**배열 생성**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGdOmZvLotQv",
        "outputId": "5a9ea413-c7eb-4f1a-f26e-f19692ef6e5c"
      },
      "source": [
        "x = token.texts_to_sequences([text])\r\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2, 3, 4, 5, 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB_iVuKzpVuv"
      },
      "source": [
        "**0과 1로 이루어진 배열로 전환**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agC692Yxo0ps",
        "outputId": "48b1905f-0b1b-423a-eceb-c2d685dde504"
      },
      "source": [
        "from keras.utils import to_categorical\r\n",
        "\r\n",
        "word_size = len(token.word_index) + 1  # 배열 맨 앞에 0이 추가되므로 단어수보다 1 많게 잡아준다.\r\n",
        "x = to_categorical(x, num_classes = word_size)\r\n",
        "\r\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG0G2Ahlp3WP"
      },
      "source": [
        "#단어 임베딩"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2CLeey0qHNl"
      },
      "source": [
        "※ 원-핫 인코딩을 그대로 사용할 때의 단점?\r\n",
        "\r\n",
        "**=> 벡터의 길이가 너무 길어질 수도 있다.**\r\n",
        "\r\n",
        "공간적 낭비를 해결하기 위해 -> **단어 임베딩!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8jBbY9hp5Lf"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\r\n",
        "from keras.layers import Embedding\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(16,4))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}